# robot-speech-intelligibility

This repository contains the dataset and code associated with the paper "No More Mumbles: Enhancing Robot Intelligibility through Speech Adaptation", by Qiaoqiao Ren, Yuanbo Hou, Thomas Demeester, and Tony Belpaeme.

In this work, we built an adaptive system that improves the robot's speech intelligibility in various spaces to adapt to different users.

Firstly, we build an annoyance level prediction model in t

To reveal how ambient sound, the environment’s acoustic quality, the distance between the user and the robot, and the user’s hearing can affect both the robot’s intelligibility and the user’s experience, we first set up a data collection campaign.

Finally, we deployed the system on a Nao social robot.

This README contains technical instructions to replicate our results. For any further questions, do not hesitate to contact Qiaoqiao[dot]Ren[at]ugent[dot]be.


## Annoyance level predicition

`## Annoyance_level_predicition/` contains the full data set. The images are selected from the [YFCC100M data set](http://www.multimediacommons.org/) and are not hosted in this repository, rather, we refer to the images using their Flickr URL.

The data set is split into a training set (3k images), validation set (500 images), and test set (500 images). Each subset is represented as a JSON file with the following members, each with one entry per image:

- **Answer.question1**: first question associated with the image
- **Answer.question2**: second question associated with the image
- **Answer.question3**: third question associated with the image
- **url**: Flickr URL of the image
- **name**: filename of the image (substring of the URL)
- **dot_string**: description of the image, generated by the dense captioning model.

We have also included a modified version of the training set, called `
train_dataset_lowfreq.json `. This data set contains all images and questions of the original, *except* for the six questions that were most common in the training set, each occurring more than 50 times. We found that the system generated more diverse questions after being trained on this reduced data set.

## Word recognition game

The image descriptions were generated by the *Dense Captioning with Joint Inference and Visual Context* model, from [https://github.com/linjieyangsc/densecap](https://github.com/linjieyangsc/densecap). We used their official sample model and did not fine-tune it further. The model ran on a machine with an NVIDIA GeForce GTX 1080 Ti GPU with 11GB VRAM.

`captioning/` contains code that we used for running the captioning model.

- `test.py` is a modified version of `lib/fast_rcnn/test.py` in the densecap repo: replace this file with our file.
- `batch.py` is a modified version of `lib/tools/demo.py` for generating captions for a batch of images.

- `caption_server.py` is a modified version of `lib/tools/demo.py` for live caption generation, e.g. when running the system with Furhat.

To run our code, first clone the original repository and follow their install instructions. Replace the `test.py`file as described above. Then, run `batch.py` with the following arguments: `python batch.py --image_folder <IMAGE_FOLDER> --output <OUTPUT_FILE> --image_file <DATASET_FILE>`. `DATASET_FILE` is a JSON file with a data set containing the `url` and `name` fields as in our data set. The script will download all images into `IMAGE_FOLDER`.

## Evaluation

`parlai_internal/` contains the code needed to train and evaluate on our task in the ParlAI framework. Check the original [ParlAI repository](https://github.com/facebookresearch/ParlAI) for more information on how to use the ParlAI framework and how to use a `parlai_internal` folder to define custom tasks.

The folder contains two tasks: `text_opener` and `text_opener_lowfreq`. They present the captions as input for the ParlAI model and the conversation-starting question as expected output. `text_opener_lowfreq` uses the training set without the six most common questions, as described above. `text_opener` uses the full data set.

The code for these tasks is based on the `mnist_qa` task included in ParlAI.

Place the data set files in a folder `data/opener_text/` in your ParlAI folder.

We used the following commands to train and evaluate the BART model on this task:

```
parlai train_model -m bart --init-model zoo:bart/bart_large/model -mf <MODEL_OUTPUT_FILE> -t internal:opener_text_lowfreq -bs 24 --fp16 true -eps 10 -lr 1e-6 --optimizer adam --inference beam --beam-size 5 --validation-every-n-epochs 8 --metrics all --validation-metric bleu-4
```
```
parlai eval_model -mf <TRAINED_MODEL_FILE> -t internal:opener_text -dt test -rf <EVALUATION_OUTPUT_FILE> --save-world-logs True --inference beam --beam-size 5
```

We trained the BART model using an NVIDIA Tesla V100 GPU, with 32GB of VRAM. Inference was done using an NVIDIA GeForce GTX 1080 Ti GPU with 11GB VRAM.

The baseline retrieval model was trained and evaluated using these commands (using only a CPU):

```
parlai train_model -m tfidf_retriever -t internal:opener_text_lowfreq -mf <MODEL_OUTPUT_FILE> -dt train:ordered -eps 1 --retriever-tokenizer simple --retriever-ngram 3
```
```
parlai eval_model -t internal:opener_text -mf <TRAINED_MODEL_FILE> -dt test --metrics all -rf <EVALUATION_OUTPUT_FILE> --save-world-logs True
```

Model weights can be made available upon request.

## Running on Furhat

Finally, you can also run a live demo of the system on a physical (or virtual) Furhat robot. For this, you need to run the following code. The python scripts require the `pyzmq` package.

- `VisualConversationStarter/` contains the skill code that should run on the Furhat. You can find instructions on how to run a skill on the robot in the [Furhat documentation](https://docs.furhat.io/skills/#running-a-skill-on-a-robot). Inspiration for the code was found [in this example skill](https://github.com/FurhatRobotics/tutorials/tree/main/camerafeed-demo).
- `captioning/captioning_server.py` should be run to generate the captions. No command-line arguments are needed. Follow the instructions above to set up the captioning model.
- For the question-generating model, run the following command in an environment where you have set up ParlAI and our task(s) as described above. Edit `parlai_internal/config.yml` so `model_file` contains the correct path to your trained model.
```
python3 ~/ParlAI/parlai/chat_service/services/browser_chat/run.py --config-path ~/ParlAI/parlai_internal/config.yml --port <QUESTION_GENERATION_PORT>
```

- `demo/comparison.txt` that integrates all components: it passes the data around between the Furhat and the models. Before running the script, make sure the IP addresses and ports for all components are correct.

## Contact

For any further questions, do not hesitate to contact Qiaoqiao[dot]Ren[at]ugent[dot]be or Yuanbo[dot]Hou[at]ugent[dot]be. You can also always raise an issue in the repository.

## Demos

Finally, you can also run a live demo of the system on a physical (or virtual) Furhat robot. For this, you need to run the following code. The python scripts require the `pyzmq` package

<!--<script> window.scroll(0,100000) </script> -->
